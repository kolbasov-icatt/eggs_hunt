{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbe25536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from env import EggsHuntEnv\n",
    "from algs_dp import *\n",
    "from algs_td import *\n",
    "from algs_vfa import *\n",
    "from algs_mc import *\n",
    "from algs_dqn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c14439-31be-4405-a2ed-49762c71d331",
   "metadata": {},
   "source": [
    "# 2 rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c067f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 36\n",
      "Terminal states:\n",
      "('Room 1', 5, 2)\n",
      "('Room 2', 5, 2)\n",
      "==================\n",
      "Possbile actions:\n",
      "search\n",
      "stay\n",
      "go_to_room_1\n",
      "go_to_room_2\n"
     ]
    }
   ],
   "source": [
    " # Maximum number of eggs for each room\n",
    "n_eggs_list =     [5,    2 ,   #4 , 7, 6,\n",
    "                  ]\n",
    "# Number of rooms\n",
    "n_rooms = len(n_eggs_list)  \n",
    "\n",
    "# Initial probabilities to find an egg for each room\n",
    "initial_p_list =  [0.8,  0.3 , #0.5, 0.9, 0.6\n",
    "                  ]\n",
    "\n",
    "# Decay probability rates for each room\n",
    "decay_rate_list = [0.8,  0.9  , #0.7, 0.6, 0.4\n",
    "                  ]\n",
    "\n",
    "GAMMA = 0.9\n",
    "\n",
    "ALPHA = 0.1\n",
    "\n",
    "# create the environment\n",
    "transition_probabilities, states, actions, total_eggs = create_environment(n_rooms, n_eggs_list, initial_p_list, decay_rate_list)\n",
    "\n",
    "# initialize the environment\n",
    "env = EggsHuntEnv(states, actions, transition_probabilities, total_eggs)\n",
    "\n",
    "env.reset()\n",
    "print(f'Number of states: {env.num_states}')\n",
    "\n",
    "print('Terminal states:')\n",
    "for state in env.states:\n",
    "    if env.is_terminal_state(state):\n",
    "        print(env.get_state_name(state))\n",
    "\n",
    "print('==================')\n",
    "print('Possbile actions:')\n",
    "for action in env.actions:\n",
    "    print(env.get_action_name(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af0a44cc-5e21-45f1-af01-02724bad2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = create_random_policy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "618ce9a7-aec5-48ce-9630-cf3225c08a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 500  # number of runs\n",
    "n_episodes = 100 # number of episodes in each run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d389291",
   "metadata": {},
   "source": [
    "## DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fd6f9b2-ea07-41e2-800c-8b14f3b907f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce317b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.26441909, 29.17797415, 28.3644854 , 23.91304622, 22.67508484,\n",
       "       21.74844964, 18.2042052 , 16.75076433, 15.66311692, 13.16226565,\n",
       "       11.39296435, 10.06917889,  8.82513738,  6.57558322,  4.892634  ,\n",
       "        5.26613558,  2.25363959,  0.        , 26.23797718, 25.26017674,\n",
       "       24.52803686, 20.5217416 , 19.40757635, 18.57360467, 15.38378468,\n",
       "       14.0756879 , 13.09680523, 11.13000234,  9.32683241,  8.062261  ,\n",
       "        8.72174006,  6.02627105,  3.4033706 ,  6.96238679,  3.6151569 ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dp, V, Q_dp = policy_iteration(env, gamma=GAMMA, threshold=0.001, verbose=False)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0dc9b62-f896-403d-928f-19dc364bd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_dp[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_dp\"\n",
    "avarage_return_dp=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6687e44e-4c82-4f60-9613-86baebcb3cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.008"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61da20",
   "metadata": {},
   "source": [
    "## MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3aaaea7-30a3-44e0-a1aa-643c690cfe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99a70722",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_mc, Q_mc= MC_onpolicy(env, policy=None, eps=0.01, num_timesteps=500, num_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27ecefdb-a064-41b3-9d37-1533ab3a43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(optimal_policy_mc[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"optimal_policy_mc\"\n",
    "avarage_return_mc=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b527397-6035-427c-8d84-997d6f394d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.952"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bc8dd",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56af63b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = temploral_difference_policy_evaluation(env, policy=random_policy, n_steps=10000, gamma=GAMMA, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be9c61-d3f2-4349-8526-8d7e4a1936b7",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1249087f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "num_runs = 300\n",
    "n_steps =  10000\n",
    "Q_list = []\n",
    "for n in range(num_runs):\n",
    "    optimal_policy, Q = sarsa(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.001, verbose=True)\n",
    "    Q_list.append(Q)\n",
    "Q_td_sarsa = np.array(Q_list).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38e51d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_td_sarsa = compute_optimal_policy(env, Q_td_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06c051a7-2068-4045-8804-3edad9dd3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_sarsa\"\n",
    "avarage_return_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ba18779-5791-4edf-9eef-0483a2c17da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.06"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b293f-eabd-489d-b0eb-01a5d2b11c31",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53446659-191d-4b75-83aa-6cd81aee1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100000\n",
    "policy_td_q_learning, Q = q_learning(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e437e937-8779-4fc7-99ac-76e6a5a4302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_td_q_learning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_q_learning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_q_learning\"\n",
    "avarage_return_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5b4e168-6496-428c-82bc-27a1366f4f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.446666666666665"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a318963",
   "metadata": {},
   "source": [
    "# VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f225da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor  = FeatureExtractor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e96e3cc7-caeb-41fb-af0b-11728b1d45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfa = VFA(env, feature_extractor, alpha=ALPHA, gamma=GAMMA, epsilon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef4948-9e27-4781-98b8-852436859381",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9484ddcc-fa6a-4fe3-9494-c7b2db529ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_sarsa = vfa.semi_gradient_sarsa(n_steps=200000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_sarsa = compute_optimal_policy(env, Q_vfa_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "754ca0bc-3811-44fc-9d30-b06b39d923e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_sarsa\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_sarsa\"\n",
    "avarage_return_VFA_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f726a5d-c4d0-4f61-9077-48dceed24d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-175.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac9b7d-c2d8-4b21-8cbb-980e9bb9d149",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6858fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_qlearning = vfa.semi_gradient_qlearning(n_steps=20000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_qlearning = compute_optimal_policy(env, Q_vfa_qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c62f8157-9889-42e2-a392-cbe509658570",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_qlearning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_qlearning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_qlearning\"\n",
    "avarage_return_VFA_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "652ea2dc-f74b-4db0-9377-98e385308452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0959d3",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0540b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DQN(env, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Test our DQN algorithm, loading it from the checkpoint file and output cumulative return.\n",
    "    \n",
    "    Args:\n",
    "        env: a MDP enviornment\n",
    "        checkpoint_path: take the file  \n",
    "    \"\"\" \n",
    "    \n",
    "    # Initialize the state, the action size and the state size\n",
    "    state= env.reset()\n",
    "    state_size = 1\n",
    "    action_size = len(env.get_possible_actions(state))\n",
    "    \n",
    "    # initialize the model\n",
    "    loaded_model = DQN(state_size, action_size)\n",
    " \n",
    "    # Load the state dictionary into the model\n",
    "    loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    loaded_model.to(device) \n",
    "\n",
    "    # initilize the agent \n",
    "    agent = DQNAgent(env, state_size, action_size)\n",
    "    agent.model = loaded_model\n",
    "    agent.target_model = loaded_model\n",
    "    agent.epsilon = 0.1\n",
    "    \n",
    "    # initialize the variables needed\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "    for _ in range(100000):# not done:\n",
    "\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "    \n",
    "    return (f'Total reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "965d9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"trained_agents/model_checkpoint_episode_300.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2077f080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total reward: 40'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DQN(env, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eae2115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the state, the action size and the state size\n",
    "state= env.reset()\n",
    "state_size = 1\n",
    "action_size = len(env.get_possible_actions(state))\n",
    "\n",
    "# initialize the model\n",
    "loaded_model = DQN(state_size, action_size)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "loaded_model.to(device) \n",
    "\n",
    "# initilize the agent \n",
    "agent = DQNAgent(env,state_size, action_size)\n",
    "agent.model = loaded_model\n",
    "agent.target_model = loaded_model\n",
    "agent.epsilon = 0.1\n",
    "\n",
    "# initialize the variables needed\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "# Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to the next state\n",
    "    env.reset()\n",
    "        \n",
    "        # check if the state is terminal\n",
    "      \n",
    "\n",
    "avarage_return_dqn=total_reward/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53d87f0e-a9b1-4c5c-8a3a-ea964895b647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.25333333333333"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374946f5-a6bc-492f-98e1-9e720d8c75d4",
   "metadata": {},
   "source": [
    "## Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55c990e1-a50e-4c17-a0fc-31bc55d0f83e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Avarage reward for 2 rooms')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(['DP','MC','TD_SARSA','TD_Q','VFA_SARSA','VFA_Q','DQN'],[avarage_return_dp, avarage_return_mc,avarage_return_sarsa, avarage_return_q, avarage_return_VFA_sarsa, avarage_return_VFA_q,avarage_return_dqn], width = 0.5)\n",
    "plt.title('Avarage reward for 2 rooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e841b13-2529-4839-bf48-09f97bbb7917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33592f3c-386a-4d7d-8ebb-e60e71596074",
   "metadata": {},
   "source": [
    "# 3 rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5906f814-4d83-487a-ac28-65c60586f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 270\n",
      "Terminal states:\n",
      "('Room 1', 5, 2, 4)\n",
      "('Room 2', 5, 2, 4)\n",
      "('Room 3', 5, 2, 4)\n",
      "==================\n",
      "Possbile actions:\n",
      "search\n",
      "stay\n",
      "go_to_room_1\n",
      "go_to_room_2\n",
      "go_to_room_3\n"
     ]
    }
   ],
   "source": [
    " # Maximum number of eggs for each room\n",
    "n_eggs_list =     [5,    2 ,   4 #, 7, 6,\n",
    "                  ]\n",
    "# Number of rooms\n",
    "n_rooms = len(n_eggs_list)  \n",
    "\n",
    "# Initial probabilities to find an egg for each room\n",
    "initial_p_list =  [0.8,  0.3 , 0.5#, 0.9, 0.6\n",
    "                  ]\n",
    "\n",
    "# Decay probability rates for each room\n",
    "decay_rate_list = [0.8,  0.9  , 0.7#, 0.6, 0.4\n",
    "                  ]\n",
    "\n",
    "GAMMA = 0.9\n",
    "\n",
    "ALPHA = 0.1\n",
    "\n",
    "# create the environment\n",
    "transition_probabilities, states, actions, total_eggs = create_environment(n_rooms, n_eggs_list, initial_p_list, decay_rate_list)\n",
    "\n",
    "# initialize the environment\n",
    "env = EggsHuntEnv(states, actions, transition_probabilities, total_eggs)\n",
    "\n",
    "env.reset()\n",
    "print(f'Number of states: {env.num_states}')\n",
    "\n",
    "print('Terminal states:')\n",
    "for state in env.states:\n",
    "    if env.is_terminal_state(state):\n",
    "        print(env.get_state_name(state))\n",
    "\n",
    "print('==================')\n",
    "print('Possbile actions:')\n",
    "for action in env.actions:\n",
    "    print(env.get_action_name(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb6706a6-82c1-4806-9aa2-38b3ea2feae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = create_random_policy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ce8d1c3-45e5-401b-80ce-bf7616c20cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 500  # number of runs\n",
    "n_episodes = 100 # number of episodes in each run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63633228-3f7b-4da7-a0b9-963a8e6aecd8",
   "metadata": {},
   "source": [
    "## DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54eeb01d-caf1-4af7-a2a8-20d0e152087a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84af9015-4b63-4a56-9746-a35e6c4bb493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.0978785 , 31.34233079, 30.56464937, 30.26614804, 30.26614804,\n",
       "       32.68102677, 30.83196224, 29.58808505, 29.17832692, 29.17832692,\n",
       "       32.40015233, 30.48840052, 29.00752475, 28.36449966, 28.36449966,\n",
       "       27.1400741 , 25.14038936, 24.25448445, 23.91426181, 23.91426181,\n",
       "       26.66477435, 24.55878236, 23.14209916, 22.67532329, 22.67532329,\n",
       "       26.34472036, 24.16740562, 22.48081798, 21.7484581 , 21.7484581 ,\n",
       "       21.99156812, 19.64441311, 18.60450925, 18.20496478, 18.20496478,\n",
       "       21.43321583, 18.9614944 , 17.29882104, 16.7509057 , 16.7509057 ,\n",
       "       21.05744094, 18.50208149, 16.52265124, 15.66312107, 15.66312107,\n",
       "       17.77167615, 14.91488485, 13.64914478, 13.16266425, 13.16266425,\n",
       "       17.09167697, 14.08346743, 12.05994358, 11.39303333, 11.39303333,\n",
       "       16.63422431, 13.52428365, 11.11526204, 10.06918039, 10.06918039,\n",
       "       14.6850948 , 11.05313461,  9.44392633,  8.82529514,  8.82529514,\n",
       "       13.82025891,  9.99595325,  7.4234994 ,  6.57560772,  6.57560772,\n",
       "       13.23862274,  9.28503214,  6.22250074,  4.8926343 ,  4.8926343 ,\n",
       "       13.11326301,  8.24962192,  6.09469692,  5.26617149,  5.26617149,\n",
       "       11.95493123,  6.83382367,  3.38908972,  2.25364415,  2.25364415,\n",
       "       11.1760219 ,  5.88181563,  1.78081473,  0.        ,  0.        ,\n",
       "       28.78809065, 27.20809772, 26.50818443, 26.23953323, 26.23953323,\n",
       "       28.41292409, 26.74876601, 25.62927654, 25.26049423, 25.26049423,\n",
       "       28.1601371 , 26.43956046, 25.10677228, 24.5280497 , 24.5280497 ,\n",
       "       23.42606669, 21.62635043, 20.829036  , 20.52283563, 20.52283563,\n",
       "       22.99829691, 21.10290413, 19.82788925, 19.40779096, 19.40779096,\n",
       "       22.71024832, 20.75066506, 19.23273618, 18.57361229, 18.57361229,\n",
       "       18.79241131, 16.6799718 , 15.74405832, 15.3844683 , 15.3844683 ,\n",
       "       18.28989425, 16.06534496, 14.56893894, 14.07581513, 14.07581513,\n",
       "       17.95169685, 15.65187334, 13.87038611, 13.09680896, 13.09680896,\n",
       "       15.75523231, 12.84375578, 11.61697677, 11.13042828, 11.13042828,\n",
       "       14.9563576 , 11.67512069,  9.99384237,  9.32690615,  9.32690615,\n",
       "       14.4814156 , 11.17185528,  9.00373584,  8.06226235,  8.06226235,\n",
       "       13.79546381, 10.77449468,  9.3405531 ,  8.72188469,  8.72188469,\n",
       "       12.9793962 ,  8.83937831,  6.87419958,  6.02629397,  6.02629397,\n",
       "       12.39704844,  7.37418427,  4.60025066,  3.40337087,  3.40337087,\n",
       "       13.11326301, 10.0030919 ,  7.88301476,  6.96241328,  6.96241328,\n",
       "       11.95493123,  7.78212943,  4.87677298,  3.61516023,  3.61516023,\n",
       "       11.1760219 ,  5.88181563,  1.78081473,  0.        ,  0.        ,\n",
       "       29.53292854, 27.20809772, 26.50818443, 26.23953323, 26.23953323,\n",
       "       29.15776198, 26.74876601, 25.62927654, 25.26049423, 25.26049423,\n",
       "       28.90497498, 26.43956046, 25.10677228, 24.5280497 , 24.5280497 ,\n",
       "       24.96627291, 21.62635043, 20.829036  , 20.52283563, 20.52283563,\n",
       "       24.53850314, 21.10290413, 19.82788925, 19.40779096, 19.40779096,\n",
       "       24.25045455, 20.75066506, 19.23273618, 18.57361229, 18.57361229,\n",
       "       21.38653862, 17.25098064, 15.74405832, 15.3844683 , 15.3844683 ,\n",
       "       20.65739699, 16.35934892, 14.56893894, 14.07581513, 14.07581513,\n",
       "       20.2238965 , 15.82928071, 13.87038611, 13.09680896, 13.09680896,\n",
       "       18.61722939, 13.86600104, 11.2842303 , 10.84639783, 10.84639783,\n",
       "       17.72940603, 12.78057892,  9.85394922,  9.25373   ,  9.25373   ,\n",
       "       17.20160112, 12.13533568,  9.00373584,  8.06226235,  8.06226235,\n",
       "       16.39753784, 11.15276737,  7.70938419,  6.94276562,  6.94276562,\n",
       "       15.53270194, 10.09558601,  6.31645545,  4.91804694,  4.91804694,\n",
       "       14.88561635,  9.30465279,  5.27439165,  3.40337087,  3.40337087,\n",
       "       15.68144246, 10.27737949,  6.55596099,  5.26617196,  5.26617196,\n",
       "       14.39437745,  8.70425358,  4.48339279,  2.25364421,  2.25364421,\n",
       "       13.52891876,  7.64646531,  3.08979583,  0.22803088,  0.        ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dp, V, Q_dp = policy_iteration(env, gamma=GAMMA, threshold=0.001, verbose=False)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "055792e2-b515-45c9-aded-420626cbc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_dp[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_dp\"\n",
    "avarage_return_dp=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ffbf082a-1941-4d8c-a2af-362ab0ca54c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.988"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7484ed2-4394-4e56-bb3c-387d13b5510c",
   "metadata": {},
   "source": [
    "## MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e390eebe-8874-4d71-a2f7-f70961be8da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "198d1cca-eaa7-4633-9272-ec5b6ec0914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_mc, Q_mc= MC_onpolicy(env, policy=None, eps=0.01, num_timesteps=500, num_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f16a89ae-42b7-48ee-87db-eb9b2fd76c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(optimal_policy_mc[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"optimal_policy_mc\"\n",
    "avarage_return_mc=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40c96e51-6348-421f-b984-3eeb88cf0117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.612"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e4b9b-ed27-40c0-9803-f7297812347a",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38148fc4-203b-4e85-a970-54db331b7285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = temploral_difference_policy_evaluation(env, policy=random_policy, n_steps=10000, gamma=GAMMA, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2df321-336b-4ec3-ad79-beb41ddca8a1",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "829b1e09-f556-4110-8818-3ff8c22854c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "num_runs = 300\n",
    "n_steps =  10000\n",
    "Q_list = []\n",
    "for n in range(num_runs):\n",
    "    optimal_policy, Q = sarsa(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.001, verbose=True)\n",
    "    Q_list.append(Q)\n",
    "Q_td_sarsa = np.array(Q_list).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eebe303f-d718-400e-b0c9-8b248a769ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_td_sarsa = compute_optimal_policy(env, Q_td_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "faf56200-5e8c-4b1a-bb08-d8cafa919a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_sarsa\"\n",
    "avarage_return_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "937fbb7e-999e-4e08-87cf-7412125b6d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.46666666666667"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15819f3f-13f5-4ac4-b451-3c340562299f",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3a79999-1b3d-4651-bf0a-6c0fb37d9d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100000\n",
    "policy_td_q_learning, Q = q_learning(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d393506c-3e0a-49d1-bfff-f628869ec2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_td_q_learning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_q_learning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_q_learning\"\n",
    "avarage_return_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c8bab5f-5f7c-4e31-8fb5-91e3c7b02e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.02"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f344ce-c369-4f22-b5b5-ecb974f55e5d",
   "metadata": {},
   "source": [
    "# VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1bf6143-4bff-458e-b3d4-42fede2628cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor  = FeatureExtractor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7709c959-dffe-41be-baeb-9674094e0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfa = VFA(env, feature_extractor, alpha=ALPHA, gamma=GAMMA, epsilon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cdc98-2bf0-4849-96b1-59f9d76f7d73",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11fa814e-60ce-4061-b120-8a636fe9cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_sarsa = vfa.semi_gradient_sarsa(n_steps=200000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_sarsa = compute_optimal_policy(env, Q_vfa_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "229503e6-9d06-41c7-b399-da454d732466",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_sarsa\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_sarsa\"\n",
    "avarage_return_VFA_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c7c6109-bb94-454c-aee0-a6f935265d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-151.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78556794-8bf1-44db-9486-886adfb0ea25",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4358f86-956a-4bd6-aa41-b5a4803f9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_qlearning = vfa.semi_gradient_qlearning(n_steps=200000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_qlearning = compute_optimal_policy(env, Q_vfa_qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4cfd10e7-d33c-4e78-96c7-df48da6f2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_qlearning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_qlearning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_qlearning\"\n",
    "avarage_return_VFA_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5f96c15-e657-454e-a485-364dbf8f8817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.12"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfddbc-eed3-48bc-8854-b2b6f9d882fb",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9eb2cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "EPISODES = 100\n",
    "num_iterations= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "166a7f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for episode 50 at 'trained_agents/model_checkpoint_episode_50.pth'\n",
      "Saved checkpoint for episode 100 at 'trained_agents/model_checkpoint_episode_100.pth'\n"
     ]
    }
   ],
   "source": [
    "Q=DQN_train(env, batch_size, EPISODES, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "be49c0b2-eb6b-49ca-b241-a6edc911f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DQN(env, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Test our DQN algorithm, loading it from the checkpoint file and output cumulative return.\n",
    "    \n",
    "    Args:\n",
    "        env: a MDP enviornment\n",
    "        checkpoint_path: take the file  \n",
    "    \"\"\" \n",
    "    \n",
    "    # Initialize the state, the action size and the state size\n",
    "    state= env.reset()\n",
    "    state_size = 1\n",
    "    action_size = len(env.get_possible_actions(state))\n",
    "    \n",
    "    # initialize the model\n",
    "    loaded_model = DQN(state_size, action_size)\n",
    " \n",
    "    # Load the state dictionary into the model\n",
    "    loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    loaded_model.to(device) \n",
    "\n",
    "    # initilize the agent \n",
    "    agent = DQNAgent(env, state_size, action_size)\n",
    "    agent.model = loaded_model\n",
    "    agent.target_model = loaded_model\n",
    "    agent.epsilon = 0.1\n",
    "    \n",
    "    # initialize the variables needed\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "    for _ in range(10000):# not done:\n",
    "\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "    \n",
    "    return (f'Total reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc887d32-66a5-4b21-a632-5368c014abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"trained_agents/model_checkpoint_episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4121dafc-7daa-425b-89fb-8ec37118a1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total reward: 26'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DQN(env, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dc31d131-b45f-4dd7-8a67-ee8494dd1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the state, the action size and the state size\n",
    "state= env.reset()\n",
    "state_size = 1\n",
    "action_size = len(env.get_possible_actions(state))\n",
    "\n",
    "# initialize the model\n",
    "loaded_model = DQN(state_size, action_size)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "loaded_model.to(device) \n",
    "\n",
    "# initilize the agent \n",
    "agent = DQNAgent(env, state_size, action_size)\n",
    "agent.model = loaded_model\n",
    "agent.target_model = loaded_model\n",
    "agent.epsilon = 0.1\n",
    "\n",
    "# initialize the variables needed\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "# Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to the next state\n",
    "    env.reset()\n",
    "        \n",
    "        # check if the state is terminal\n",
    "      \n",
    "\n",
    "avarage_return_dqn=total_reward/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a9a0f9ce-d59e-4f13-a8fa-4abdc0336531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.26"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb33a30-6dcd-44ba-875f-cf04ada4e2d0",
   "metadata": {},
   "source": [
    "## Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e224bbd2-7cb6-4dbc-8c28-abdc82c32fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Avarage reward for 3 rooms')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(['DP','MC','TD_SARSA','TD_Q','VFA_SARSA','VFA_Q','DQN'],[avarage_return_dp, avarage_return_mc,avarage_return_sarsa, avarage_return_q, avarage_return_VFA_sarsa, avarage_return_VFA_q,avarage_return_dqn], width = 0.5)\n",
    "plt.title('Avarage reward for 3 rooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736888fa-72cb-4c2a-bbda-8b18989f8aae",
   "metadata": {},
   "source": [
    "# 4 rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "93a4ff2f-5885-47e0-8a2f-d0d0cb21298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 2880\n",
      "Terminal states:\n",
      "('Room 1', 5, 2, 4, 7)\n",
      "('Room 2', 5, 2, 4, 7)\n",
      "('Room 3', 5, 2, 4, 7)\n",
      "('Room 4', 5, 2, 4, 7)\n",
      "==================\n",
      "Possbile actions:\n",
      "search\n",
      "stay\n",
      "go_to_room_1\n",
      "go_to_room_2\n",
      "go_to_room_3\n",
      "go_to_room_4\n"
     ]
    }
   ],
   "source": [
    " # Maximum number of eggs for each room\n",
    "n_eggs_list =     [5,    2 ,   4 , 7#, 6,\n",
    "                  ]\n",
    "# Number of rooms\n",
    "n_rooms = len(n_eggs_list)  \n",
    "\n",
    "# Initial probabilities to find an egg for each room\n",
    "initial_p_list =  [0.8,  0.3 , 0.5, 0.9#, 0.6\n",
    "                  ]\n",
    "\n",
    "# Decay probability rates for each room\n",
    "decay_rate_list = [0.8,  0.9  ,0.7, 0.6 #, 0.4\n",
    "                  ]\n",
    "\n",
    "GAMMA = 0.9\n",
    "\n",
    "ALPHA = 0.1\n",
    "\n",
    "# create the environment\n",
    "transition_probabilities, states, actions, total_eggs = create_environment(n_rooms, n_eggs_list, initial_p_list, decay_rate_list)\n",
    "\n",
    "# initialize the environment\n",
    "env = EggsHuntEnv(states, actions, transition_probabilities, total_eggs)\n",
    "\n",
    "env.reset()\n",
    "print(f'Number of states: {env.num_states}')\n",
    "\n",
    "print('Terminal states:')\n",
    "for state in env.states:\n",
    "    if env.is_terminal_state(state):\n",
    "        print(env.get_state_name(state))\n",
    "\n",
    "print('==================')\n",
    "print('Possbile actions:')\n",
    "for action in env.actions:\n",
    "    print(env.get_action_name(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "93cfdf63-b743-41df-b912-157e78d2beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = create_random_policy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b589a6ca-6ca2-4164-bc12-d1ab080c0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 500  # number of runs\n",
    "n_episodes = 100 # number of episodes in each run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db958aac-fe1d-48a9-aa66-87781789af4a",
   "metadata": {},
   "source": [
    "## DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f942e972-2ee7-413b-9579-75bff356f6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "da8b90f1-b9d5-4a1c-9347-98f07115d436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37.52637222, 34.42158312, 33.36682346, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dp, V, Q_dp = policy_iteration(env, gamma=GAMMA, threshold=0.001, verbose=False)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "46ce34ba-fedf-4770-b450-6b806d858d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_dp[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_dp\"\n",
    "avarage_return_dp=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ef92f36-a3c2-44f3-b2ab-5a3484d4cf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.42"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0b4d1-0833-4225-a1ab-5cb60dadb9b5",
   "metadata": {},
   "source": [
    "## MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d5068898-64c7-4c39-9a4e-6f63d25d7454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a21139c5-d206-4589-8d63-37cb94e5f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_mc, Q_mc= MC_onpolicy(env, policy=None, eps=0.01, num_timesteps=500, num_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eeceb4bc-6085-4ab0-a716-8d0c5f6d9b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(optimal_policy_mc[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"optimal_policy_mc\"\n",
    "avarage_return_mc=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e384983d-726a-4062-b748-e360a4b0ca59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.136"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b58808-ea9a-4ca9-aa3e-6d838adb6b00",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "845689ec-0d18-4842-b1db-5411cb144744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = temploral_difference_policy_evaluation(env, policy=random_policy, n_steps=10000, gamma=GAMMA, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c7890-e8d7-481a-b616-d62146f5ebde",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4b8231da-a15d-48f4-bde8-c86fdf41d5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "num_runs = 300\n",
    "n_steps =  10000\n",
    "Q_list = []\n",
    "for n in range(num_runs):\n",
    "    optimal_policy, Q = sarsa(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.001, verbose=True)\n",
    "    Q_list.append(Q)\n",
    "Q_td_sarsa = np.array(Q_list).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "16c6f388-b5a4-4380-8e7f-6a9fa169b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_td_sarsa = compute_optimal_policy(env, Q_td_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88ff1767-648b-49a1-b1ab-6dd0e761dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_sarsa\"\n",
    "avarage_return_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "64290e9a-8d9e-4fc0-a0ce-edae494e32b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.31333333333333"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e827c7-28dd-4cb3-b15a-ef9ad1f2c0fd",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8e61cd87-6634-4633-a637-f064004cd3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100000\n",
    "policy_td_q_learning, Q = q_learning(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "51dc9ac1-47d6-4302-a525-88705cb267ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_td_q_learning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_q_learning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_q_learning\"\n",
    "avarage_return_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6974820d-7012-423e-8a45-4773bebcd73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.46666666666667"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d9130-edcd-4e32-a7c1-c98ed32f3f2b",
   "metadata": {},
   "source": [
    "# VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1cc7235d-f274-46d5-8ab6-24113d700ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor  = FeatureExtractor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6eeb047b-e24b-4602-b35d-f83c714fa3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfa = VFA(env, feature_extractor, alpha=ALPHA, gamma=GAMMA, epsilon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683430a-5eee-4844-8833-6b71b311de3f",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "89f5ce34-da28-4eb1-ad4c-6104c1642626",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_sarsa = vfa.semi_gradient_sarsa(n_steps=200000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_sarsa = compute_optimal_policy(env, Q_vfa_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "402c5454-ba06-4114-9602-0aa715d1c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_sarsa\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_sarsa\"\n",
    "avarage_return_VFA_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "216aa673-cb8b-4180-a9fe-e220f00d5e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-104.16"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91250bf-682a-4e87-8507-6ca2fe82ac94",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63a321b9-9397-4495-a36e-b92a6c1d6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_qlearning = vfa.semi_gradient_qlearning(n_steps=20000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_qlearning = compute_optimal_policy(env, Q_vfa_qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8909ca37-8021-4b45-8903-3e046ef7fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_qlearning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_qlearning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_qlearning\"\n",
    "avarage_return_VFA_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ace0ce2d-680b-436b-9899-f782e9db78ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-140.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fdc61-5641-4872-8494-ac30944fe566",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1cabc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "EPISODES = 100\n",
    "num_iterations= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c8e2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for episode 50 at 'trained_agents/model_checkpoint_episode_50.pth'\n",
      "Saved checkpoint for episode 100 at 'trained_agents/model_checkpoint_episode_100.pth'\n"
     ]
    }
   ],
   "source": [
    "Q=DQN_train(env, batch_size, EPISODES, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "72fdda0c-fcf1-4679-84dc-259f591963ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DQN(env, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Test our DQN algorithm, loading it from the checkpoint file and output cumulative return.\n",
    "    \n",
    "    Args:\n",
    "        env: a MDP enviornment\n",
    "        checkpoint_path: take the file  \n",
    "    \"\"\" \n",
    "    \n",
    "    # Initialize the state, the action size and the state size\n",
    "    state= env.reset()\n",
    "    state_size = 1\n",
    "    action_size = len(env.get_possible_actions(state))\n",
    "    \n",
    "    # initialize the model\n",
    "    loaded_model = DQN(state_size, action_size)\n",
    " \n",
    "    # Load the state dictionary into the model\n",
    "    loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    loaded_model.to(device) \n",
    "\n",
    "    # initilize the agent \n",
    "    agent = DQNAgent(env, state_size, action_size)\n",
    "    agent.model = loaded_model\n",
    "    agent.target_model = loaded_model\n",
    "    agent.epsilon = 0.1\n",
    "    \n",
    "    # initialize the variables needed\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "    for _ in range(10000):# not done:\n",
    "\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "    \n",
    "    return (f'Total reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f81a6fdd-3f62-4356-912d-9f87571daf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"trained_agents/model_checkpoint_episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3eb1d01e-e742-4a15-a85f-057d5849b9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total reward: 8'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DQN(env, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "61e4dc70-c032-48c6-97a4-34a9b0f3c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the state, the action size and the state size\n",
    "state= env.reset()\n",
    "state_size = 1\n",
    "action_size = len(env.get_possible_actions(state))\n",
    "\n",
    "# initialize the model\n",
    "loaded_model = DQN(state_size, action_size)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "loaded_model.to(device) \n",
    "\n",
    "# initilize the agent \n",
    "agent = DQNAgent(env, state_size, action_size)\n",
    "agent.model = loaded_model\n",
    "agent.target_model = loaded_model\n",
    "agent.epsilon = 0.1\n",
    "\n",
    "# initialize the variables needed\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "# Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to the next state\n",
    "    env.reset()\n",
    "        \n",
    "        # check if the state is terminal\n",
    "      \n",
    "\n",
    "avarage_return_dqn=total_reward/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d474363d-8863-4832-8f0d-aee80098b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e54276-589b-460d-86cf-b57a6fb392b7",
   "metadata": {},
   "source": [
    "## Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5402623e-835f-428c-9400-16b8bd5fd168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Avarage reward for 4 rooms')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(['DP','MC','TD_SARSA','TD_Q','VFA_SARSA','VFA_Q','DQN'],[avarage_return_dp, avarage_return_mc,avarage_return_sarsa, avarage_return_q, avarage_return_VFA_sarsa, avarage_return_VFA_q,avarage_return_dqn], width = 0.5)\n",
    "plt.title('Avarage reward for 4 rooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2898e-71ac-4c8d-85e1-2a862972bc58",
   "metadata": {},
   "source": [
    "# 5 rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cf68d9ad-f062-4052-94a0-8fcefa90e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 25200\n",
      "Terminal states:\n",
      "('Room 1', 5, 2, 4, 7, 6)\n",
      "('Room 2', 5, 2, 4, 7, 6)\n",
      "('Room 3', 5, 2, 4, 7, 6)\n",
      "('Room 4', 5, 2, 4, 7, 6)\n",
      "('Room 5', 5, 2, 4, 7, 6)\n",
      "==================\n",
      "Possbile actions:\n",
      "search\n",
      "stay\n",
      "go_to_room_1\n",
      "go_to_room_2\n",
      "go_to_room_3\n",
      "go_to_room_4\n",
      "go_to_room_5\n"
     ]
    }
   ],
   "source": [
    " # Maximum number of eggs for each room\n",
    "n_eggs_list =     [5,    2 ,   4 , 7, 6,\n",
    "                  ]\n",
    "# Number of rooms\n",
    "n_rooms = len(n_eggs_list)  \n",
    "\n",
    "# Initial probabilities to find an egg for each room\n",
    "initial_p_list =  [0.8,  0.3 , 0.5, 0.9, 0.6\n",
    "                  ]\n",
    "\n",
    "# Decay probability rates for each room\n",
    "decay_rate_list = [0.8,  0.9  , 0.7, 0.6, 0.4\n",
    "                  ]\n",
    "\n",
    "GAMMA = 0.9\n",
    "\n",
    "ALPHA = 0.1\n",
    "\n",
    "# create the environment\n",
    "transition_probabilities, states, actions, total_eggs = create_environment(n_rooms, n_eggs_list, initial_p_list, decay_rate_list)\n",
    "\n",
    "# initialize the environment\n",
    "env = EggsHuntEnv(states, actions, transition_probabilities, total_eggs)\n",
    "\n",
    "env.reset()\n",
    "print(f'Number of states: {env.num_states}')\n",
    "\n",
    "print('Terminal states:')\n",
    "for state in env.states:\n",
    "    if env.is_terminal_state(state):\n",
    "        print(env.get_state_name(state))\n",
    "\n",
    "print('==================')\n",
    "print('Possbile actions:')\n",
    "for action in env.actions:\n",
    "    print(env.get_action_name(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "db520940-a82a-422f-adf7-e2e45d6aec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = create_random_policy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ead33170-8739-4482-ba19-5e644e0b7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 500  # number of runs\n",
    "n_episodes = 100 # number of episodes in each run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd584b4-b459-46b0-bcee-848a5d01a4d0",
   "metadata": {},
   "source": [
    "## DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2ede68a2-183a-4b42-a4a7-b7bf823a729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8b035700-f579-45ce-a826-85755f26fdad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.28514517, 37.56935241, 37.52766352, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dp, V, Q_dp = policy_iteration(env, gamma=GAMMA, threshold=0.001, verbose=False)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a661e7ca-90fc-4707-9c16-5c81a52023a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_dp[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_dp\"\n",
    "avarage_return_dp=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "02095356-c4d4-4cbc-a8ab-e6890d7dc6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.16"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb8278-56cb-41cf-8efb-648b8423bd20",
   "metadata": {},
   "source": [
    "## MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dc06ebde-6ed2-4adf-9c96-9f9bd1a25757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "db594520-3bb3-4229-80af-db4b79bb3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_mc, Q_mc= MC_onpolicy(env, policy=None, eps=0.01, num_timesteps=500, num_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c57c7571-40e0-489f-826e-5829a80e3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(optimal_policy_mc[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"optimal_policy_mc\"\n",
    "avarage_return_mc=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7b8e6ed2-03d4-4791-8727-a22b31db7287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.324"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a35977-4aef-44d6-a607-4923e4a95dfd",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d0a22e43-b05f-4057-9ce6-3d23eb804d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = temploral_difference_policy_evaluation(env, policy=random_policy, n_steps=10000, gamma=GAMMA, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce6e1d-1b7c-4d35-be9a-db800192007e",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4f2880bc-34f0-4e49-988f-8b960e33cce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n",
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "num_runs = 300\n",
    "n_steps =  100000\n",
    "Q_list = []\n",
    "for n in range(num_runs):\n",
    "    optimal_policy, Q = sarsa(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.001, verbose=True)\n",
    "    Q_list.append(Q)\n",
    "Q_td_sarsa = np.array(Q_list).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0798a5b9-0989-497c-97bd-159c24527577",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_td_sarsa = compute_optimal_policy(env, Q_td_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4b161da0-dab0-476f-805e-ef770fbafebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# This is for one policy\n",
    "total_return = 0 \n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_sarsa\"\n",
    "avarage_return_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bf88ec1e-f869-4dbc-a6a1-6ef15faf7cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.49333333333333"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10acd04-065a-4485-9e73-e6beb2fec86d",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b3b73340-c399-4964-b4e7-13ede7eb0d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed 0 steps.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100000\n",
    "policy_td_q_learning, Q = q_learning(env, n_steps=n_steps, gamma=GAMMA, alpha=ALPHA, begin_epsilon=1, end_epsilon=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "643faef8-470e-4f74-ac44-2572bf587f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_td_q_learning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_td_q_learning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_td_q_learning\"\n",
    "avarage_return_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "eb128af9-f557-4db8-ac09-78c845bd8e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.60666666666667"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a86dfc-51bc-47b4-a235-807e2a8cf19f",
   "metadata": {},
   "source": [
    "# VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f0239a39-b038-4950-a5fe-a1adc3e7fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor  = FeatureExtractor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5c4a0c85-6600-4fd4-8226-021e431ce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfa = VFA(env, feature_extractor, alpha=ALPHA, gamma=GAMMA, epsilon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1fd22-a595-450b-90c0-ca42a9dfc197",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a91edd7f-3e9d-409b-a70e-ca6b1c3773a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_sarsa = vfa.semi_gradient_sarsa(n_steps=200000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_sarsa = compute_optimal_policy(env, Q_vfa_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "68897fa8-e44a-4d6a-9546-756f7ff03532",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_sarsa\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_sarsa[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            #print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_sarsa\"\n",
    "avarage_return_VFA_sarsa=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "af0c1bb5-a854-41d6-9517-586cb2b1a959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-140.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913f696-88d3-4be9-82f3-98e66fbac495",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "30351f3f-fc08-4d56-9fcc-9dda334503e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vfa_qlearning = vfa.semi_gradient_qlearning(n_steps=20000, begin_epsilon=1, end_epsilon=0.1)\n",
    "policy_vfa_qlearning = compute_optimal_policy(env, Q_vfa_qlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a9a0b902-0615-4f34-b4ea-6126a04e802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# This is for one policy policy_vfa_qlearning\n",
    "total_return = 0\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        action = np.argmax(policy_vfa_qlearning[env.current_state])\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            print(f'Completed in {ep} episodes')\n",
    "            break\n",
    "        total_return += reward  # return for one run, but they can be different so we need to average them over many independent runs\n",
    "    env.reset()\n",
    "        \n",
    "# average return of the policy \"policy_vfa_qlearning\"\n",
    "avarage_return_VFA_q=total_return/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "31a50e41-33ff-450c-9cc9-6ee17eaf1c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-140.0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_VFA_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd26c99-2e1b-47bc-9c5c-be750a603ea6",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2de5f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "EPISODES = 100\n",
    "num_iterations= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e72f7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for episode 50 at 'trained_agents/model_checkpoint_episode_50.pth'\n",
      "Saved checkpoint for episode 100 at 'trained_agents/model_checkpoint_episode_100.pth'\n"
     ]
    }
   ],
   "source": [
    "Q=DQN_train(env, batch_size, EPISODES, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "38500f12-ef26-4d50-b567-8f84d3119619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DQN(env, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Test our DQN algorithm, loading it from the checkpoint file and output cumulative return.\n",
    "    \n",
    "    Args:\n",
    "        env: a MDP enviornment\n",
    "        checkpoint_path: take the file  \n",
    "    \"\"\" \n",
    "    \n",
    "    # Initialize the state, the action size and the state size\n",
    "    state= env.reset()\n",
    "    state_size = 1\n",
    "    action_size = len(env.get_possible_actions(state))\n",
    "    \n",
    "    # initialize the model\n",
    "    loaded_model = DQN(state_size, action_size)\n",
    " \n",
    "    # Load the state dictionary into the model\n",
    "    loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    loaded_model.to(device) \n",
    "\n",
    "    # initilize the agent \n",
    "    agent = DQNAgent(env, state_size, action_size)\n",
    "    agent.model = loaded_model\n",
    "    agent.target_model = loaded_model\n",
    "    agent.epsilon = 0.1\n",
    "    \n",
    "    # initialize the variables needed\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "    for _ in range(10000):# not done:\n",
    "\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "    \n",
    "    return (f'Total reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "50419dff-2c0f-4f7d-905e-7e29c80ebaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"trained_agents/model_checkpoint_episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "833a4678-7a72-42fe-a0cd-358864b02553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total reward: 0'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DQN(env, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0df0c058-545f-4e0a-a3aa-fdf24bd41b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the state, the action size and the state size\n",
    "state= env.reset()\n",
    "state_size = 1\n",
    "action_size = len(env.get_possible_actions(state))\n",
    "\n",
    "# initialize the model\n",
    "loaded_model = DQN(state_size, action_size)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "loaded_model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "loaded_model.to(device) \n",
    "\n",
    "# initilize the agent \n",
    "agent = DQNAgent(env, state_size, action_size)\n",
    "agent.model = loaded_model\n",
    "agent.target_model = loaded_model\n",
    "agent.epsilon = 0.1\n",
    "\n",
    "# initialize the variables needed\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "# Loop until the episode ends ( we can't do it, it's too computational expensive, so we put a threshold)\n",
    "for n in range(num_runs):\n",
    "    for ep in range(n_episodes):\n",
    "        # Reshape the state for the DQN\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        # Select an action following the greedy policy\n",
    "        action = agent.act(state, training=False)\n",
    "\n",
    "        # Perform the action and store the next_state the reward and if the state is terminal\n",
    "        reward, next_state, done = env.step(action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # update reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to the next state\n",
    "    env.reset()\n",
    "        \n",
    "        # check if the state is terminal\n",
    "      \n",
    "\n",
    "avarage_return_dqn=total_reward/ num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "252a43d2-bf01-4bf8-b2c0-b2bb0ab1f38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_return_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491c57b-55a8-4d45-998e-27fb9e1744d8",
   "metadata": {},
   "source": [
    "## Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "227a5379-0c9e-4338-8ed3-2bea98a17a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Avarage reward for 5 rooms')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(['DP','MC','TD_SARSA','TD_Q','VFA_SARSA','VFA_Q','DQN'],[avarage_return_dp, avarage_return_mc,avarage_return_sarsa, avarage_return_q, avarage_return_VFA_sarsa, avarage_return_VFA_q,avarage_return_dqn], width = 0.5)\n",
    "plt.title('Avarage reward for 5 rooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd35c12-568a-4501-b3ed-d19bca992f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
